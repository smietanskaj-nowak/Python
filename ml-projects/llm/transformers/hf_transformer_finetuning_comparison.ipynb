Istotność architektury modeli w oznaczaniu danych.
Analiza dokładności i czasu treningu, na które wpływ ma rozmiar modelu i stopień jego skomplikowania.
!pip install -q datasets transformers evaluate scikit-learn

from datasets import load_dataset
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          TrainingArguments, Trainer, DataCollatorWithPadding)
import evaluate, torch, numpy as np, time, gc
from datasets import Dataset, DatasetDict

def flatten_squad(articles):

    ctx, qst, ans = [], [], []
    for art in articles:
        for para in art["paragraphs"]:
            context = para["context"]
            for qa in para["qas"]:
                # lista odpowiedzi może być pusta
                if qa.get("answers"):
                    texts  = [a["text"]         for a in qa["answers"]]
                    starts = [a["answer_start"] for a in qa["answers"]]
                else:
                    texts, starts = [], []      # pytania bez odpowiedzi

                ctx.append(context)
                qst.append(qa["question"])
                ans.append({"text": texts, "answer_start": starts})
    return ctx, qst, ans
!wget https://huggingface.co/datasets/clarin-pl/poquad/raw/main/poquad-dev.json
!wget https://huggingface.co/datasets/clarin-pl/poquad/resolve/main/poquad-train.json
from datasets import load_dataset, DatasetDict

poquad = load_dataset(
    "json",
    data_files={"train": "poquad-train.json",
                "dev":   "poquad-dev.json"},
    field="data"
)
Zamieniam listę artykułów POQuAD ➜ trzy listy: context, question, answers
- 'answers' ma dokładnie pola wymagane przez HF (text[], answer_start[])
- pytania bez odpowiedzi dostają pustą listę (zgodne ze SQuAD 2.0)
import json

with open("poquad-train.json") as f:
    train_data = json.load(f)["data"]

with open("poquad-dev.json") as f:
    dev_data = json.load(f)["data"]

print("Articles (train):", len(train_data))
print("Articles (dev)  :", len(dev_data))
train_ctx, train_q, train_a = flatten_squad(train_data)
dev_ctx,   dev_q,   dev_a   = flatten_squad(dev_data)

train_dataset = Dataset.from_dict(
    {"context": train_ctx, "question": train_q, "answers": train_a})
dev_dataset   = Dataset.from_dict(
    {"context": dev_ctx,   "question": dev_q,   "answers": dev_a})

poquad = DatasetDict({"train": train_dataset, "dev": dev_dataset})
print(poquad)
MODELS = {
    "HerBERT-base": "allegro/herbert-base-cased",
    "mBERT-base"  : "bert-base-multilingual-cased",
}
Zapętlenie treningu: tokenizacja → trening 1 epoki → ewaluacja
def flatten_squad_to_clf(articles):
    rows = []
    for art in articles:
        for para in art["paragraphs"]:
            ctx = para["context"]
            for qa in para["qas"]:
                rows.append({
                    "text":    qa["question"] + " " + ctx,
                    "label":   int(qa["is_impossible"])
                })
    return rows

train_rows = flatten_squad_to_clf(train_data)
dev_rows   = flatten_squad_to_clf(dev_data)

from datasets import Dataset, DatasetDict
train_dataset = Dataset.from_dict({k: [r[k] for r in train_rows] for k in ["text", "label"]})
dev_dataset   = Dataset.from_dict({k: [r[k] for r in dev_rows]   for k in ["text", "label"]})

dataset = DatasetDict({"train": train_dataset, "test": dev_dataset})
print(dataset)
!pip install sacremoses
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)
import evaluate, numpy as np, torch, time, gc

MODELS = {
    "HerBERT-base": "allegro/herbert-base-cased",
    "mBERT-base":   "bert-base-multilingual-cased",
}

results = {}

for nick, ckpt in MODELS.items():
    print(f"\n===  {nick}  ===")
    tok = AutoTokenizer.from_pretrained(ckpt)

    # tokenizacja zbioru
    def tok_fn(batch):
        return tok(batch["text"], truncation=True, max_length=256)
    tok_ds = dataset.map(tok_fn, batched=True, remove_columns=["text"])

    collator = DataCollatorWithPadding(tok, pad_to_multiple_of=8)
    model = AutoModelForSequenceClassification.from_pretrained(
        ckpt, num_labels=2
    )

    # TrainingArguments (API 4.51)
    args = TrainingArguments(
        output_dir=f"run-{nick}",
        num_train_epochs=1,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        learning_rate=2e-5,
        eval_strategy="epoch",
        fp16=torch.cuda.is_available(),
        logging_steps=50,
        report_to="none",
        seed=42,
    )

    # metryka
    metric_acc = evaluate.load("accuracy")
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        preds = np.argmax(logits, axis=-1)
        return metric_acc.compute(predictions=preds, references=labels)

    # Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tok_ds["train"],
        eval_dataset=tok_ds["test"],
        tokenizer=tok,
        data_collator=collator,
        compute_metrics=compute_metrics,
    )

    start = time.time()
    trainer.train()
    elapsed = (time.time() - start) / 60

    res = trainer.evaluate()
    results[nick] = {"accuracy": res["eval_accuracy"],
                     "time_min": round(elapsed, 1)}

    # czyszczenie pamięci między modelami
    del model, trainer, tok_ds
    torch.cuda.empty_cache()
    gc.collect()

print("\nWyniki")
for k, v in results.items():
    print(f"{k:<15}  acc={v['accuracy']:.3f}   {v['time_min']} min")
Test LIME
!pip install -q lime
from lime.lime_text import LimeTextExplainer
import matplotlib.pyplot as plt
import torch, numpy as np
import glob, os, pprint

ckpts = sorted(glob.glob("run-HerBERT-base/checkpoint-*"))
print("Znalezione checkpointy:", ckpts)
BEST_DIR = "run-HerBERT-base/checkpoint-7078"
tokenizer_lime = AutoTokenizer.from_pretrained(BEST_DIR)
model_lime     = AutoModelForSequenceClassification.from_pretrained(BEST_DIR)
def lime_classifier(texts):
    enc = tokenizer_lime(texts,
                         padding=True,
                         truncation=True,
                         max_length=256,
                         return_tensors="pt").to(model_lime.device)

    with torch.no_grad():
        logits = model_lime(**enc).logits
    return torch.softmax(logits, dim=-1).cpu().numpy()
idx = 42                      # dowolny indeks z części testowej
sample_text  = dataset["test"]["text"][idx]
true_label   = dataset["test"]["label"][idx]

print("Tekst:", sample_text[:250], "…")
print("Prawdziwa etykieta:", true_label)
explainer = LimeTextExplainer(
    class_names=[str(i) for i in sorted(set(dataset["train"]["label"]))]
)

exp = explainer.explain_instance(
    sample_text,
    lime_classifier,
    num_features=10,     # ile tokenów pokazać
    top_labels=1,        # interesuje nas ta etykieta, którą model przewidział
)

fig = exp.as_pyplot_figure(label=exp.top_labels[0])
plt.title("Tokeny wpływające na predykcję")
plt.show()
Na wykresie:

zielone słupki – tokeny, które podnoszą prawdopodobieństwo prognozowanej klasy.
czerwone słupki – tokeny, które obniżają to prawdopodobieństwo.
Im dłuższy słupek, tym silniejszy (pozytywny lub negatywny) wpływ danego słowa lub fragmentu.
