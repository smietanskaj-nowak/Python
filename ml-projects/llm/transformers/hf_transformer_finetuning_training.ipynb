# Fine-tuning transformera

## Weryfikacja dostępności GPU


[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apohllo/sztuczna-inteligencja/blob/master/lab5/lab_5.ipynb)


!nvidia-smi
## Podpięcie dysku Google (opcjonalne)
from google.colab import drive
drive.mount('/content/gdrive')
## Przygotowanie danych do klasyfikacji

Przygotowanie danych rozpoczniemy od sklonowania repozytorium zawierającego pytania i odpowiedzi.
!pip install -U datasets fsspec


from datasets import load_dataset

dataset = load_dataset("clarin-pl/poquad", split="train")

print(dataset[0])
dataset
Zbiór danych jest podzielony na dwie części: treningową i walidacyjną. Rozmiar części treningowej to ponad 46 tysięcy pytań i odpowiedzi, natomiast części walidacyjnej to ponad 5 tysięcy pytań i odpowiedzi.
from datasets import load_dataset

dataset = load_dataset("clarin-pl/poquad")

train_dataset = dataset["train"]
dev_dataset = dataset["validation"]

print(f"Train: {len(train_dataset)}, Dev: {len(dev_dataset)}")
dataset['train']['answers'][:5]
dataset['train']['question'][:5]
!wget https://huggingface.co/datasets/clarin-pl/poquad/raw/main/poquad-dev.json
!wget https://huggingface.co/datasets/clarin-pl/poquad/resolve/main/poquad-train.json
Dla bezpieczeństwa, jeśli korzystamy z Google drive, to przeniesiemy pliki na nasz dysk:
!mkdir gdrive/MyDrive/poquad
!mv poquad-dev.json gdrive/MyDrive/poquad
!mv poquad-train.json gdrive/MyDrive/poquad

!head -30 gdrive/MyDrive/poquad/poquad-dev.json
## Ładowanie danych
import json

path = "gdrive/MyDrive/poquad"  # Ścieżka do folderu (trzeba ją sobie dostosować)

# Odczyt danych treningowych
with open(path + "/poquad-train.json") as input:
    train_data = json.loads(input.read())["data"]

print(f"Train data articles: {len(train_data)}")

# Odczyt danych walidacyjnych
with open(path + "/poquad-dev.json") as input:
    dev_data = json.loads(input.read())["data"]

print(f"Dev data articles: {len(dev_data)}")

# Liczba pytań w danych treningowych i walidacyjnych
print(f"Train questions: {sum([len(e['paragraphs'][0]['qas']) for e in train_data])}")
print(f"Dev questions: {sum([len(e['paragraphs'][0]['qas']) for e in dev_data])}")
all_contexts = [e["paragraphs"][0]["context"] for e in train_data] + [
    e["paragraphs"][0]["context"] for e in dev_data
]
import random


tuples = [[], []]

for idx, dataset in enumerate([train_data, dev_data]):
    for data in dataset:
        context = data["paragraphs"][0]["context"]
        for question_answers in data["paragraphs"][0]["qas"]:
            question = question_answers["question"]
            if question_answers["is_impossible"]:
                tuples[idx].append(
                    {
                        "text": f"Pytanie: {question} Kontekst: {context}",
                        "label": 0,
                    }
                )
            else:
                tuples[idx].append(
                    {
                        "text": f"Pytanie: {question} Kontekst: {context}",
                        "label": 1,
                    }
                )
                while True:
                    negative_context = random.choice(all_contexts)
                    if negative_context != context:
                        tuples[idx].append(
                            {
                                "text": f"Pytanie: {question} Kontekst: {negative_context}",
                                "label": 0,
                            }
                        )
                        break

train_tuples, dev_tuples = tuples
print(f"Total count in train/dev: {len(train_tuples)}/{len(dev_tuples)}")
print(
    f"Positive count in train/dev: {sum([e['label'] for e in train_tuples])}/{sum([e['label'] for e in dev_tuples])}"
)
Widzimy, że uzyskane zbiory danych cechują się dość dobrym zbalansowaniem.

Dobrą praktyką po wprowadzeniu zmian w zbiorze danych, jest wyświetlenie kilku przykładowych punktów danych, w celu wykrycia ewentualnych błędów, które powstały na etapie konwersji zbioru. Pozwala to uniknąć nieprzyjemnych niespodzianek, np. stworzenie identycznego zbioru danych testowych i treningowych.
print(train_tuples[0:1])
print(dev_tuples[0:1])
from datasets import Dataset, DatasetDict

train_dataset = Dataset.from_list(train_tuples)
dev_dataset = Dataset.from_list(dev_tuples)
datasets = DatasetDict({"train": train_dataset, "dev": dev_dataset})
datasets.save_to_disk(path + "/question-context-classification")
!pip install sacremoses
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(
    "allegro/herbert-base-cased",
    num_labels=2
)

pl_tokenizer = AutoTokenizer.from_pretrained("allegro/herbert-base-cased")

def tokenize_function(examples):
    return pl_tokenizer(examples["text"], padding='do_not_pad', truncation=True)


tokenized_datasets = datasets.map(tokenize_function, batched=True)
tokenized_datasets["train"]
example = tokenized_datasets["train"][0]
print(example["text"])
print("-" * 60)
print(example["input_ids"])
print("-" * 60)
print(example["attention_mask"])
print("|".join(pl_tokenizer.convert_ids_to_tokens(list(example["input_ids"]))))
print(len([e for e in example["input_ids"] if e != 1]))
print(len([e for e in example["attention_mask"] if e == 1]))
## Trening z użyciem transformersów


model = AutoModelForSequenceClassification.from_pretrained(
    "allegro/herbert-base-cased", num_labels=2
)

model
for name, param in model.named_parameters():
    print(name)
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_name = "allegro/herbert-base-cased"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 1. Zamroź wszystko
for p in model.parameters():
    p.requires_grad_(False)

# 2. Odmroź ostatni blok encodera
for p in model.bert.encoder.layer[-1].parameters():
    p.requires_grad_(True)

# 3. Odmroź pooler (jeśli jest) i warstwę klasyfikatora
for p in model.bert.pooler.parameters():
    p.requires_grad_(True)
for p in model.classifier.parameters():
    p.requires_grad_(True)

# 4.  Kontrola
trainable = [n for n, p in model.named_parameters() if p.requires_grad]
print("Warstwy trenowane:", trainable)
def print_trainable_parameters(model):
    trainable_params = 0
    all_params = 0
    for name, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    trainable = 100 * trainable_params / all_params
    print(
        f"trainable params: {trainable_params:,d} || all params: {all_params:,d} "
        f"|| trainable%: {trainable:.4f}%"
    )
    return trainable

trainable_proportion = print_trainable_parameters(model)
assert 5 < trainable_proportion < 7
print("Solution correct!")
#import os
#os.environ["WANDB_DISABLED"] = "true"
from transformers import TrainingArguments
import numpy as np

arguments = TrainingArguments(
    output_dir=path + "/output",
    do_train=True,
    do_eval=True,
    eval_strategy="steps",
    eval_steps=100,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    num_train_epochs=1,
    logging_first_step=True,
    logging_strategy="steps",
    logging_steps=50,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=1,
    metric_for_best_model="accuracy",
    fp16=True,
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    seed=42,
    load_best_model_at_end=True,
    label_smoothing_factor=0.1,
    group_by_length=True,
    eval_on_start=True,
)
pip install evaluate
import evaluate

metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=1)
    return metric.compute(predictions=predictions, references=labels)
from transformers import AutoModelForSequenceClassification

model_name = "allegro/herbert-base-cased"
num_labels = 2

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)

print("Aktualny typ modelu:", type(model))
from transformers import Trainer, DataCollatorWithPadding

seed = 42
train_examples_count = len(tokenized_datasets["train"])
print(train_examples_count)
dev_examples_count = len(tokenized_datasets["dev"])
print(dev_examples_count)

trainer = Trainer(
    model=model,
    args=arguments,
    train_dataset=tokenized_datasets["train"].select(range(train_examples_count)).shuffle(seed=seed),
    eval_dataset=tokenized_datasets["dev"].select(range(dev_examples_count)).shuffle(seed=seed),
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer=pl_tokenizer)
)
!mkdir -p ./output/runs
%load_ext tensorboard
%tensorboard --logdir gdrive/MyDrive/poquad/output/runs
#%tensorboard --logdir ./output/runs
print("Aktualny typ modelu:", type(model))
# 3m @ 4080
trainer.train()
WTest jakości predykcji udzielanych przez model na bazie losowej strony Wikipedii.
from transformers import AutoModelForQuestionAnswering, AutoTokenizer
import torch

# Załaduj model i tokenizator
model_name = "allegro/herbert-base-cased"
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ustawienie modelu w tryb inferencji (wyłączenie dropoutu)
model.eval()

# Skopiowany fragment tekstu z Wikipedii
context = """
Jest to fragment tekstu z Wikipedii, który będziesz używać do zadawania pytań. Możesz wprowadzić tutaj
dowolny fragment z artykułu, który wybierzesz na Wikipedii. Pamiętaj, aby zachować odpowiedni kontekst.
"""

# Pytania, na które można odpowiedzieć na podstawie tekstu
questions_with_answer = [
    "Jakie jest główne hasło artykułu?",
    "Jakie są kluczowe informacje zawarte w artykule?",
    "Jakie są powiązane tematy lub kategorie artykułu?"
]

# Pytania, na które nie można odpowiedzieć na podstawie tekstu
questions_without_answer = [
    "Jakie są aktualne wydarzenia związane z tematem artykułu?",
    "Jakie są subiektywne opinie na temat omawianego zagadnienia?",
    "Jakie są najnowsze badania dotyczące tematu artykułu?"
]

# Funkcja do odpowiadania na pytania
def answer_question(question, context):
    inputs = tokenizer(question, context, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
        answer_start = torch.argmax(outputs.start_logits)  # Początek odpowiedzi
        answer_end = torch.argmax(outputs.end_logits) + 1  # Koniec odpowiedzi
        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][answer_start:answer_end]))
    return answer

# Testowanie modelu na pytaniach z odpowiedzią
print("Pytania, na które można odpowiedzieć:")
for question in questions_with_answer:
    print(f"Q: {question}")
    answer = answer_question(question, context)
    print(f"A: {answer}\n")

# Testowanie modelu na pytaniach bez odpowiedzi
print("Pytania, na które nie można odpowiedzieć:")
for question in questions_without_answer:
    print(f"Q: {question}")
    answer = answer_question(question, context)
    print(f"A: {answer}\n")
