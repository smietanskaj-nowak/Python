# Prompt engineering modeli LLM w Ollama
## 1. Przygotowanie Å›rodowiska

1. **Instalacja Ollama**  
   - macOS / Linux:  
     ```bash
     curl -fsSL https://ollama.com/install.sh | sh
     ```  
   - Windows (PowerShell):  
     ```powershell
     iwr https://ollama.com/install.ps1 -useb | iex
     ```

2. **Uruchomienie usÅ‚ugi**  
   ```bash
   ollama serve
   ```

3. **Pobranie przykÅ‚adowego modelu** (tu: `gemma2:2b`)  
   ```bash
   ollama pull gemma2:2b
   ```



!pip install ollama
import ollama

# Testowe zapytanie
response = ollama.chat(
    model='gemma2:2b',
    messages=[{'role': 'user', 'content': 'Podaj 3 ciekawostki o Å¼ubrach'}]
)
print(response['message']['content'])

---
## Test technik prompotwania

### Zero-shot prompting
text = """Stitch, znany jako Eksperyment 626, zostaÅ‚ stworzony przez szalonego naukowca JumbÄ™ JookibÄ™ jako istota o destrukcyjnych skÅ‚onnoÅ›ciach. Po ucieczce z niewoli kosmicznej lÄ…duje na Hawajach, gdzie spotyka Lilo. Dziewczynka, pragnÄ…c towarzystwa, przygarnia go jako psa, nadajÄ…c mu imiÄ™ Stich.

Lilo mieszka z siostrÄ… Nani, ktÃ³ra po Å›mierci rodzicÃ³w stara siÄ™ zapewniÄ‡ jej opiekÄ™. Relacje miÄ™dzy siostrami sÄ… napiÄ™te, zwÅ‚aszcza Å¼e Nani ma trudnoÅ›ci z utrzymaniem pracy, a pracownik socjalny Cobra Bubbles ocenia ich sytuacjÄ™. Pojawienie siÄ™ Sticha wprowadza dodatkowy chaos, ale takÅ¼e staje siÄ™ katalizatorem zbliÅ¼enia siÄ™ rodziny.

W miÄ™dzyczasie kosmiczne wÅ‚adze wysyÅ‚ajÄ… JumbÄ™ oraz agenta Plikleya na ZiemiÄ™, by pojmali Sticha. Ich nieudolne prÃ³by schwytania go prowadzÄ… do serii komicznych i niebezpiecznych sytuacji, ktÃ³re zagraÅ¼ajÄ… bezpieczeÅ„stwu Lilo i Nani.

Ostatecznie Stitch, poczÄ…tkowo zaprogramowany do destrukcji, uczy siÄ™ wartoÅ›ci rodziny i przyjaÅºni dziÄ™ki Lilo. Razem stawiajÄ… czoÅ‚a przeciwnoÅ›ciom, udowadniajÄ…c, Å¼e wiÄ™zi rodzinne sÄ… silniejsze niÅ¼ jakiekolwiek zagroÅ¼enia zewnÄ™trzne."""

prompt = f"""Return a JSON with one key "summary" containing
a 30â€‘word English summary of the following text:

{text}
"""

response = ollama.chat(
    model='gemma2:2b',
    messages=[{'role': 'user', 'content': prompt}]
)
print(response['message']['content'])

from pydantic import BaseModel
import ollama

class CityWeather(BaseModel):
    city: str
    temp_c: float
    condition: str | None

response = ollama.chat(
    model='gemma2:2b',
    messages=[{'role':'user','content':'Weather in Warsaw today.'}],
    format=CityWeather.model_json_schema(),   # â† schema trafia do Ollamy
    options={'temperature':0}
)

weather = CityWeather.model_validate_json(response['message']['content'])
print(weather)
import ollama, json

text = """Stitch, znany jako Eksperyment 626, zostaÅ‚ stworzony przez szalonego naukowca JumbÄ™ JookibÄ™ jako istota o destrukcyjnych skÅ‚onnoÅ›ciach. Po ucieczce z niewoli kosmicznej lÄ…duje na Hawajach, gdzie spotyka Lilo. Dziewczynka, pragnÄ…c towarzystwa, przygarnia go jako psa, nadajÄ…c mu imiÄ™ Stich.

Lilo mieszka z siostrÄ… Nani, ktÃ³ra po Å›mierci rodzicÃ³w stara siÄ™ zapewniÄ‡ jej opiekÄ™. Relacje miÄ™dzy siostrami sÄ… napiÄ™te, zwÅ‚aszcza Å¼e Nani ma trudnoÅ›ci z utrzymaniem pracy, a pracownik socjalny Cobra Bubbles ocenia ich sytuacjÄ™. Pojawienie siÄ™ Sticha wprowadza dodatkowy chaos, ale takÅ¼e staje siÄ™ katalizatorem zbliÅ¼enia siÄ™ rodziny.

W miÄ™dzyczasie kosmiczne wÅ‚adze wysyÅ‚ajÄ… JumbÄ™ oraz agenta Plikleya na ZiemiÄ™, by pojmali Sticha. Ich nieudolne prÃ³by schwytania go prowadzÄ… do serii komicznych i niebezpiecznych sytuacji, ktÃ³re zagraÅ¼ajÄ… bezpieczeÅ„stwu Lilo i Nani.

Ostatecznie Stitch, poczÄ…tkowo zaprogramowany do destrukcji, uczy siÄ™ wartoÅ›ci rodziny i przyjaÅºni dziÄ™ki Lilo. Razem stawiajÄ… czoÅ‚a przeciwnoÅ›ciom, udowadniajÄ…c, Å¼e wiÄ™zi rodzinne sÄ… silniejsze niÅ¼ jakiekolwiek zagroÅ¼enia zewnÄ™trzne."""

prompt = (
    'You are a JSON API. Respond **only** with valid JSON, no markdown.\n'
    'Return a JSON object with one key "summary" that holds a 30-word English summary '
    'of the following text.\n\n'
    f'{text}'
)

response = ollama.chat(
    model='gemma2:2b',
    messages=[{'role': 'user', 'content': prompt}],
    format='json',                 
    options={'temperature': 0}     
)

result = json.loads(response['message']['content'])
print(result)

from pydantic import BaseModel
import ollama, json

class OneLineSummary(BaseModel):
    summary: str

response = ollama.chat(
    model='gemma2:2b',
    messages=[{
        'role': 'user',
        'content': (
            'Summarise the text below in exactly 30 English words.\n\n' + text
        )
    }],
    format=OneLineSummary.model_json_schema(),  
    options={'temperature': 0}
)

summary_obj = OneLineSummary.model_validate_json(response['message']['content'])
print(summary_obj.summary)
---
### Few-shot prompting
examples = [
    {"role": "user", "content": "Translate to emoji: I love programming"},
    {"role": "assistant", "content": "ğŸ’»â¤ï¸"},
    {"role": "user", "content": "Translate to emoji: Fire and ice"},
    {"role": "assistant", "content": "ğŸ”¥â„ï¸"},
    {"role": "user", "content": "Translate to emoji: Peace and coffee. Running and winning"}
]

response = ollama.chat(model='gemma2:2b', messages=examples)
print(response['message']['content'])

shots = [
    {"role": "user",   "content": "I absolutely love this phone â€” battery lasts all day!"},
    {"role": "assistant", "content": "positive"},
    {"role": "user",   "content": "Awesome support, my ticket had been closed in 3 hours."},
    {"role": "assistant", "content": "positive"},
    {"role": "user",   "content": "Horrible customer service, my ticket is there for 3 weeks, already."},
    {"role": "assistant", "content": "negative"},
]

query = {"role": "user", "content": "Itâ€™s okay, nothing special but works as advertised."}

response = ollama.chat(
    model="gemma2:2b",
    messages = shots + [query],
    options  = {"temperature": 0}
)

print("Predykcja:", response["message"]["content"])
schema = """Return valid JSON: { "name": <string>, "title": <string>, "company": <string> }"""

shots = [
    # przykÅ‚ad 1
    {"role": "user",      "content": schema + "\nâ€” Sarah Connors-Newman â€” Director of Operations at Skynet Industries"},
    {"role": "assistant", "content": '{"name":"Sarah Connors-Newman","title":"Director of Operations","company":"Skynet Industries"}'},
    # przykÅ‚ad 2
    {"role": "user",      "content": schema + "\nâ€” Dr. Hiro Tanaka, Lead Scientist â€¢ QuantumX"},
    {"role": "assistant", "content": '{"name":"Hiro Tanaka","title":"Lead Scientist","company":"QuantumX"}'},
    # przykÅ‚ad 3
    {"role": "user",      "content": schema + "\nâ€” Prof. dr hab. Janek Dzbanek Przewodnik Wszystkich AGH"},
    {"role": "assistant", "content": '{"name":"Janek Dzbanek","title":"Przewodnik Wszystkich","company":"AGH"}'},
]

text = "â€¢ Prof. dr hab. Janek Tanaka WysadziÅ‚ AGH w powietrze"

response = ollama.chat(
    model="gemma2:2b",
    messages=shots + [
        {"role": "user", "content": schema + "\n" + text}
    ],
    format="json",              
    options={"temperature": 0}
)

print(json.loads(response["message"]["content"]))
import textwrap

SYSTEM = "You write polite, concise 3-sentence email replies."

shots = [
    {"role":"system","content":SYSTEM},
    # przykÅ‚ad 1
    {"role":"user", "content":"Client: Can we postpone the meeting to Friday?"},
    {"role":"assistant","content":textwrap.dedent("""\
        Hello John,
        Friday works perfectly for me. Looking forward to our discussion.
        Best regards, Anna""")},
    # przykÅ‚ad 2
    {"role":"user", "content":"Client: Could you send the revised contract by tomorrow?"},
    {"role":"assistant","content":textwrap.dedent("""\
        Hi Maria,
        Iâ€™ll send the updated contract first thing tomorrow morning.
        Kind regards, Anna""")},
]

new_mail = "Client: We need a brief project timeline before next Monday."

resp = ollama.chat(
    model="gemma2:2b",
    messages=shots + [{"role":"user","content":new_mail}],
    options={"temperature":0.4}
)

print(resp["message"]["content"])
---
## Role i wiadomoÅ›ci systemowe
messages = [
    {"role": "system", "content": "You are a strict but encouraging Polish language teacher."},
    {"role": "user", "content": "Popraw proszÄ™ to zdanie: 'Wczoraj byÅ‚em w kinie i oglÄ…dali film.'"}
]

response = ollama.chat(model='gemma2:2b', messages=messages)
print(response['message']['content'])

---
## Chainâ€‘ofâ€‘Thought (CoT)
puzzle = """Jestem liczbÄ… dwucyfrowÄ…. Moja suma cyfr to 9, a po odwrÃ³ceniu cyfr jestem o 27 mniejsza. JakÄ… liczbÄ… jestem?"""

prompt = f"""Let's solve this stepâ€‘byâ€‘step.

{puzzle}

Format:
STEPÂ 1: ...
STEPÂ 2: ...
ANSWER: <number>
"""

response = ollama.chat(model='gemma2:2b', messages=[{'role':'user','content':prompt}])
print(response['message']['content'])

puzzle = (
    "Jestem liczbÄ… dwucyfrowÄ…. Moja suma cyfr to 9, "
    "a po odwrÃ³ceniu cyfr jestem o 27 mniejsza. JakÄ… liczbÄ… jestem?"
)

SYSTEM = (
    "You are a careful math tutor. "
    "Explain EVERY algebraic step explicitly, without skipping or merging steps. "
    "Never combine two operations into one line; label them as separate STEP n."
)

USER = f"""
Solve the puzzle **step-by-step**.

{puzzle}

Output format (exactly):
STEP 1: â€¦
STEP 2: â€¦
â€¦
ANSWER: <number>
Do NOT skip, abbreviate, or summarise any step.
"""

resp = ollama.chat(
    model="gemma2:2b",
    messages=[
        {"role": "system", "content": SYSTEM},
        {"role": "user",   "content": USER}, 
    ]
)

print(resp["message"]["content"])
---
## Ewaluacja promptÃ³w
### Selfâ€‘critique


rubric = """Please critique the following answer on a scale 1â€‘5 for correctness and completeness.
Give JSON: {"score": <1â€‘5>, "comment": "..."}"""

evaluation = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "user", "content": rubric + "\n\nANSWER:\n" + response['message']['content']}
    ]
)
print(evaluation['message']['content'])
### NER (Named Entity Recognition)


SYSTEM = "You are an information extractor."

prompt = "Return valid JSON listing every PERSON, LOCATION and ORGANIZATION, DATE that appears."

text = """7 lutego 1919, dekretem Tymczasowego Naczelnika PaÅ„stwa JÃ³zefa PiÅ‚sudskiego, utworzona zostaÅ‚a Pocztowa Kasa OszczÄ™dnoÅ›ci. Jej pierwszym dyrektorem zostaÅ‚ mianowany 28 grudnia 1919 Hubert Linde. Po nim prezesami PKO byli Emil Schmidt i Henryk Gruber. Z czasem utworzono centralÄ™ banku w Warszawie z siedzibÄ… przy ul. ÅšwiÄ™tokrzyskiej 31/33 oraz pierwsze oddziaÅ‚y lokalne: w Krakowie, Lwowie, Åodzi, Poznaniu i Katowicach. Pierwszym celem PKO staÅ‚o siÄ™ wprowadzenie do obiegu polskiego zÅ‚otego zamiast marki polskiej (jako pochodnej marki niemieckiej). Od 1920 bank posiadaÅ‚ osobowoÅ›Ä‡ prawnÄ…, jako instytucja paÅ„stwowa. Pracownicy Kasy byli zrzeszeni w Zrzeszeniu PracownikÃ³w Pocztowej Kasy OszczÄ™dnoÅ›ci, ktÃ³re miaÅ‚o swoje koÅ‚a przy wiÄ™kszych OddziaÅ‚ach, np. w Warszawie, w Åodzi."""

output_format = """
{
  "PERSON": ["<imiÄ™ nazwisko>"],
  "LOCATION": ["<miejsce>"],
  "ORG": ["<organizacja>"]
}
"""

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": f"{prompt}\n\nText:\n{text}\n\nOutput format:\n{output_format}"}
    ]
)
print(response['message']['content'])
SYSTEM = "You are a precise NER engine. Output ONLY valid JSON as specified."

shots = [
    {
        "role": "user",
        "content": "Text: \"Barack Obama studied at Columbia University in New York.\""
    },
    {
        "role": "assistant", 
        "content": "{\"PERSON\":[\"Barack Obama\"],\"LOCATION\":[\"New York\"],\"ORG\":[\"Columbia University\"]}"
    },
    {
        "role": "user",
        "content": "Text: \"Lech WaÅ‚Ä™sa wspÃ³Å‚tworzyÅ‚ SolidarnoÅ›Ä‡ w GdaÅ„sku.\""
    },
    {
        "role": "assistant",
        "content": "{\"PERSON\":[\"Lech WaÅ‚Ä™sa\"],\"LOCATION\":[\"GdaÅ„sk\"],\"ORG\":[\"SolidarnoÅ›Ä‡\"]}"
    },
    {
        "role": "user", 
        "content": "Text: \"{tu_tekst}}\""
    }
]

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": f"{prompt}\n\nText:\n{text}\n\nOutput format:\n{output_format}"}
    ]
)
print(response['message']['content'])

### Analiza wydÅºwiÄ™ku


SYSTEM = "JesteÅ› precyzyjnym analizatorem wydÅºwiÄ™ku tekstu. Zwracaj wyÅ‚Ä…cznie poprawny JSON wedÅ‚ug podanej specyfikacji."

prompt = """OkreÅ›l wydÅºwiÄ™k (pozytywny/negatywny/neutralny) poniÅ¼szej recenzji.
ZwrÃ³Ä‡ JSON z dwoma kluczami: "sentiment" i "evidence" (zacytuj decydujÄ…cy fragment).

Recenzja:
Gdy pierwszy raz uniosÅ‚am do nosa butelkÄ™ Szamponu â€LÅ›niÄ…ca Naturaâ€, ogarnÄ™Å‚a mnie fala wspomnieÅ„ z wakacji nad BaÅ‚tykiem â€“ zapach morskiej bryzy splecionej z nutami sÅ‚odkiej pomaraÅ„czy i soczystych malin. JuÅ¼ samo otwarcie opakowania byÅ‚o jak zdjÄ™cie wiecznego klosza z codziennoÅ›ci: w jednej sekundzie Å‚azienka zamieniÅ‚a siÄ™ w rozÅ›wietlonÄ…, letniÄ… plaÅ¼Ä™, a ja â€“ w beztroskÄ… dziewczynÄ™ z wiatrem we wÅ‚osach.

GÄ™sta, perÅ‚owa formuÅ‚a wypÅ‚ywa z butelki niczym pÅ‚ynne Å›wiatÅ‚o. Kiedy rozprowadzam jÄ… na wilgotnych pasmach, mam wraÅ¼enie, Å¼e kaÅ¼dy kosmyk wita jÄ… z zachwytem: pianÄ™ miÄ™kkÄ… jak pianka z latte, ktÃ³ra lekko skrzypi miÄ™dzy palcami i otula skÃ³rÄ™ gÅ‚owy kojÄ…cym chÅ‚odem. To nie jest zwykÅ‚e mycie wÅ‚osÃ³w â€“ to rytuaÅ‚, w ktÃ³rym czujÄ™ siÄ™ dopieszczona od cebulek aÅ¼ po same koÅ„ce.

JuÅ¼ podczas spÅ‚ukiwania sÅ‚yszÄ™ charakterystyczny, czysty â€skrzypâ€ zdrowych wÅ‚osÃ³w. StrumieÅ„ wody odbija Å›wiatÅ‚o, a moje pasma â€“ lÅ›niÄ…ce i lekkie â€“ taÅ„czÄ… w nim jak jedwabne wstÄ…Å¼ki. Nie mogÄ™ siÄ™ oprzeÄ‡, by nie zanurzyÄ‡ dÅ‚oni w tej tafli â€“ gÅ‚adkoÅ›Ä‡ rozczarowuje mnie tylko w jednym: Å¼e nie da siÄ™ jej zapisaÄ‡ na staÅ‚e w pamiÄ™ci dotyku.

Po wysuszeniu czujÄ™ siÄ™, jakbym stÄ…paÅ‚a po czerwonym dywanie: pukle sypkie, sprÄ™Å¼yste, unoszÄ…ce siÄ™ przy kaÅ¼dym ruchu gÅ‚owy. Aromat, ktÃ³ry pozostaje, przypomina delikatny perfum â€“ dyskretny, ale wystarczajÄ…co wyrazisty, by ktoÅ› obok zapytaÅ‚ z zaciekawieniem: â€Czym pachniesz?â€ Wtedy uÅ›miecham siÄ™ szeroko, bo wiem, Å¼e ten sekret kryje siÄ™ w niewielkiej, zielonej butelce stojÄ…cej na pÃ³Å‚ce.

Szampon â€LÅ›niÄ…ca Naturaâ€ to dla mnie nie tylko kosmetyk; to codzienny list miÅ‚osny do moich wÅ‚osÃ³w â€“ powiew odwagi i czuÅ‚oÅ›ci zarazem. JeÅ›li Twoje pasma pragnÄ… rozgwieÅ¼dÅ¼onego blasku, a Twoje zmysÅ‚y tÄ™skniÄ… za chwilÄ… szczerej przyjemnoÅ›ci, pozwÃ³l temu szamponowi szepnÄ…Ä‡ im historiÄ™ o tym, jak piÄ™kno rodzi siÄ™ z zachwytu.

"""

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": prompt}
    ]
)
print(response['message']['content'])
SYSTEM = "JesteÅ› precyzyjnym analizatorem wydÅºwiÄ™ku tekstu. Zwracaj wyÅ‚Ä…cznie poprawny JSON wedÅ‚ug podanej specyfikacji."

shots = [
    {
        "role": "user",
        "content": "Tekst: \"Åšwietny telefon, bardzo polecam!\""
    },
    {
        "role": "assistant",
        "content": "{\"sentiment\":\"positive\",\"evidence\":\"Åšwietny telefon, bardzo polecam!\"}"
    },
    {
        "role": "user",
        "content": "Tekst: \"Niestety jakoÅ›Ä‡ wykonania pozostawia wiele do Å¼yczenia.\""
    },
    {
        "role": "assistant",
        "content": "{\"sentiment\":\"negative\",\"evidence\":\"jakoÅ›Ä‡ wykonania pozostawia wiele do Å¼yczenia\"}"
    },
    {
        "role": "user",
        "content": "Tekst: No spoko dziaÅ‚a, ale ekran bardzo delikatny i mÃ³gÅ‚by siÄ™ Å‚atwo porysowaÄ‡."
    }
]

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        *shots
    ]
)
print(response['message']['content'])
### Ekstrakcja relacji


SYSTEM = "JesteÅ› precyzyjnym analizatorem tekstu. ZnajdÅº wszystkie relacje gdzie osoba pracuje dla organizacji. ZwrÃ³Ä‡ tablicÄ™ JSON z obiektami {\"person\":\"<imiÄ™>\",\"company\":\"<organizacja>\"}."

shots = [
    {
        "role": "user", 
        "content": "Tekst: \"Jan Kowalski pracuje w Microsoft jako programista.\""
    },
    {
        "role": "assistant",
        "content": "[{\"person\":\"Jan Kowalski\",\"company\":\"Microsoft\"}]"
    },
    {
        "role": "user",
        "content": "Tekst: \"Anna Kowalska teraz pracuje na AGH, ale kiedyÅ› karierÄ™ robiÅ‚a w Google.\""
    }
]

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        *shots
    ]
)
print(response['message']['content'])
