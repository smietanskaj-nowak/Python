# Konfiguracja ≈õrodowiska Google¬†Colab z¬†Ollama


#@title üõ†Ô∏è Instalacja i¬†uruchomienie Ollama
%%bash
set -e
echo "‚û°Ô∏è Aktualizacja pakiet√≥w systemowych‚Ä¶"
sudo apt-get -yq update && sudo apt-get -yq install curl git-lfs > /dev/null

echo "‚û°Ô∏è Instalacja binarki Ollama‚Ä¶"
curl -fsSL https://ollama.com/install.sh | sudo bash > /dev/null

echo "‚û°Ô∏è Instalacja klienta Python‚Ä¶"
pip install -q --upgrade ollama

echo "‚û°Ô∏è Uruchamianie serwera Ollama (w tle)‚Ä¶"
nohup ollama serve > /dev/null 2>&1 &

# Pobierz przyk≈Çadowy model ‚Äì¬†zmie≈Ñ na w≈Çasny, je≈õli chcesz
MODEL_NAME="mistral"
echo "‚û°Ô∏è Pobieranie modelu $MODEL_NAME ‚Ä¶ (to mo≈ºe chwilƒô potrwaƒá)"
ollama pull $MODEL_NAME

echo "‚úì Ollama is ready."

# Promptowanie modeli LLM w Ollama
## 1. Przygotowanie ≈õrodowiska

1. **Instalacja Ollama**  
   - macOS / Linux:  
     ```bash
     curl -fsSL https://ollama.com/install.sh | sh
     ```  
   - Windows (PowerShell):  
     ```powershell
     iwr https://ollama.com/install.ps1 -useb | iex
     ```

2. **Uruchomienie us≈Çugi**  
   ```bash
   ollama serve
   ```

3. **Pobranie przyk≈Çadowego modelu** (tu: `gemma2:2b`)  
   ```bash
   ollama pull gemma2:2b
   ```

> Modele LLM sƒÖ du≈ºe (> 4‚ÄØGB). Upewnij siƒô, ≈ºe masz wystarczajƒÖco wolnego miejsca na dysku. Na poczƒÖtek sugerujƒô u≈ºycie jak najmniejszych modeli (wspomniana **gemma2:2b** to nieca≈Çe 4GB).

# (opcjonalnie) Instalacja klienta Python
# Je≈õli pracujesz w ≈õrodowisku wirtualnym JupyterLab:
!pip install ollama
import ollama

# Testowe zapytanie
response = ollama.chat(
    model='gemma2:2b',
    messages=[{'role': 'user', 'content': 'Podaj 3 ciekawostki o ≈ºubrach'}]
)
print(response['message']['content'])

---
### Zero-shot prompting
text = """przyk≈Çadowy tekst"""

prompt = f"""Return a JSON with one key "summary" containing
a 30‚Äëword English summary of the following text:

{text}
"""

response = ollama.chat(
    model='gemma2:2b',
    messages=[{'role': 'user', 'content': prompt}]
)
print(response['message']['content'])

from pydantic import BaseModel
import ollama

class CityWeather(BaseModel):
    city: str
    temp_c: float
    condition: str | None

response = ollama.chat(
    model='gemma2:2b',
    messages=[{'role':'user','content':'Weather in Warsaw today.'}],
    format=CityWeather.model_json_schema(),   # ‚Üê schema trafia do Ollamy
    options={'temperature':0}
)

weather = CityWeather.model_validate_json(response['message']['content'])
print(weather)
import ollama, json

text = """przyk≈Çadowy tekst"""

prompt = (
    'You are a JSON API. Respond **only** with valid JSON, no markdown.\n'
    'Return a JSON object with one key "summary" that holds a 30-word English summary '
    'of the following text.\n\n'
    f'{text}'
)

response = ollama.chat(
    model='gemma2:2b',
    messages=[{'role': 'user', 'content': prompt}],
    format='json',                 
    options={'temperature': 0}     
)

result = json.loads(response['message']['content'])
print(result)

from pydantic import BaseModel
import ollama, json

class OneLineSummary(BaseModel):
    summary: str

response = ollama.chat(
    model='gemma2:2b',
    messages=[{
        'role': 'user',
        'content': (
            'Summarise the text below in exactly 30 English words.\n\n' + text
        )
    }],
    format=OneLineSummary.model_json_schema(),  
    options={'temperature': 0}
)

summary_obj = OneLineSummary.model_validate_json(response['message']['content'])
print(summary_obj.summary)
---
### Few-shot prompting
examples = [
    {"role": "user", "content": "Translate to emoji: I love programming"},
    {"role": "assistant", "content": "üíª‚ù§Ô∏è"},
    {"role": "user", "content": "Translate to emoji: Fire and ice"},
    {"role": "assistant", "content": "üî•‚ùÑÔ∏è"},
    {"role": "user", "content": "Translate to emoji: Peace and coffee"}
]

response = ollama.chat(model='gemma2:2b', messages=examples)
print(response['message']['content'])

shots = [
    {"role": "user",   "content": "I absolutely love this phone ‚Äî battery lasts all day!"},
    {"role": "assistant", "content": "positive"},
    {"role": "user",   "content": "Terrible support, my ticket is still open after 3 weeks."},
    {"role": "assistant", "content": "negative"},
]

query = {"role": "user", "content": "It‚Äôs okay, nothing special but works as advertised."}

response = ollama.chat(
    model="gemma2:2b",
    messages = shots + [query],
    options  = {"temperature": 0}
)

print("Predykcja:", response["message"]["content"])
schema = """Return valid JSON: { "name": <string>, "title": <string>, "company": <string> }"""

shots = [
    # przyk≈Çad 1
    {"role": "user",      "content": schema + "\n‚Äî Sarah Connors ‚Äî Director of Operations at Skynet Industries"},
    {"role": "assistant", "content": '{"name":"Sarah Connors","title":"Director of Operations","company":"Skynet Industries"}'},
    # przyk≈Çad 2
    {"role": "user",      "content": schema + "\n‚Äî Dr. Hiro Tanaka, Lead Scientist ‚Ä¢ QuantumX"},
    {"role": "assistant", "content": '{"name":"Hiro Tanaka","title":"Lead Scientist","company":"QuantumX"}'},
]

text = "‚Ä¢ John O‚ÄôReilly ‚Üí Senior Product Manager @ NovaTech"

response = ollama.chat(
    model="gemma2:2b",
    messages=shots + [
        {"role": "user", "content": schema + "\n" + text}
    ],
    format="json",              
    options={"temperature": 0}
)

print(json.loads(response["message"]["content"]))
import textwrap

SYSTEM = "You write polite, concise 3-sentence email replies."

shots = [
    {"role":"system","content":SYSTEM},
    # przyk≈Çad 1
    {"role":"user", "content":"Client: Can we postpone the meeting to Friday?"},
    {"role":"assistant","content":textwrap.dedent("""\
        Hello John,
        Friday works perfectly for me. Looking forward to our discussion.
        Best regards, Anna""")},
    # przyk≈Çad 2
    {"role":"user", "content":"Client: Could you send the revised contract by tomorrow?"},
    {"role":"assistant","content":textwrap.dedent("""\
        Hi Maria,
        I‚Äôll send the updated contract first thing tomorrow morning.
        Kind regards, Anna""")},
]

new_mail = "Client: We need a brief project timeline before next Monday."

resp = ollama.chat(
    model="gemma2:2b",
    messages=shots + [{"role":"user","content":new_mail}],
    options={"temperature":0.4}
)

print(resp["message"]["content"])
messages = [
    {"role": "system", "content": "You are a strict but encouraging Polish language teacher."},
    {"role": "user", "content": "Popraw prosze to zdanie: 'Wczoraj by≈Çem w kinie i oglƒÖdali film.'"}
]

response = ollama.chat(model='gemma2:2b', messages=messages)
print(response['message']['content'])

---
## Chain‚Äëof‚ÄëThought (CoT)
puzzle = """Jestem liczbƒÖ dwucyfrowƒÖ. Moja suma cyfr to 9, a po odwr√≥ceniu cyfr jestem o 27 mniejsza. JakƒÖ liczbƒÖ jestem?"""

prompt = f"""Let's solve this step‚Äëby‚Äëstep.

{puzzle}

Format:
STEP¬†1: ...
STEP¬†2: ...
ANSWER: <number>
"""

response = ollama.chat(model='gemma2:2b', messages=[{'role':'user','content':prompt}])
print(response['message']['content'])

puzzle = """Jestem liczbƒÖ dwucyfrowƒÖ. Moja suma cyfr to 9, a po odwr√≥ceniu
cyfr jestem o 27 mniejsza. JakƒÖ liczbƒÖ jestem?"""

SYSTEM = (                                  #	Ma najwy≈ºszy priorytet ‚Äì model bƒôdzie pr√≥bowa≈Ç spe≈Çniƒá nakazy nawet kosztem d≈Çugo≈õci odpowiedzi.
    "You are a careful math tutor. "
    "Explain EVERY algebraic step explicitly, without skipping or merging steps. "
    "Never combine two operations into one line; label them as separate STEP n."
)

USER = f"""
Solve the puzzle **step-by-step**.

{puzzle}

Output format (exactly):
STEP 1: ‚Ä¶
STEP 2: ‚Ä¶
‚Ä¶
ANSWER: <number>
Do NOT skip, abbreviate, or summarise any step.
"""

resp = ollama.chat(
    model="gemma2:2b",
    messages=[
        {"role": "system", "content": SYSTEM},
)

print(resp["message"]["content"])
)

print(resp["message"]["content"])
---
## Ewaluacja prompt√≥w
### Self‚Äëcritique



rubric = """Please critique the following answer on a scale 1‚Äë5 for correctness and completeness.
Give JSON: {"score": <1‚Äë5>, "comment": "..."}"""

evaluation = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "user", "content": rubric + "\n\nANSWER:\n" + response['message']['content']}
    ]
)
print(evaluation['message']['content'])
### NER (Named Entity Recognition)


SYSTEM = "You are an information extractor."

prompt = "Return valid JSON listing every PERSON, LOCATION and ORGANIZATION that appears."

text = "{tu_tekst}"

output_format = """
{
  "PERSON": ["<imiƒô nazwisko>"],
  "LOCATION": ["<miejsce>"],
  "ORG": ["<organizacja>"]
}
"""

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": f"{prompt}\n\nText:\n{text}\n\nOutput format:\n{output_format}"}
    ]
)
print(response['message']['content'])
SYSTEM = "You are a precise NER engine. Output ONLY valid JSON as specified."

shots = [
    {
        "role": "user",
        "content": "Text: \"Barack Obama studied at Columbia University in New York.\""
    },
    {
        "role": "assistant", 
        "content": "{\"PERSON\":[\"Barack Obama\"],\"LOCATION\":[\"New York\"],\"ORG\":[\"Columbia University\"]}"
    },
    {
        "role": "user",
        "content": "Text: \"Lech Wa≈Çƒôsa wsp√≥≈Çtworzy≈Ç Solidarno≈õƒá w Gda≈Ñsku.\""
    },
    {
        "role": "assistant",
        "content": "{\"PERSON\":[\"Lech Wa≈Çƒôsa\"],\"LOCATION\":[\"Gda≈Ñsk\"],\"ORG\":[\"Solidarno≈õƒá\"]}"
    },
    {
        "role": "user", 
        "content": "Text: \"{tu_tekst}}\""
    }
]

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": f"{prompt}\n\nText:\n{text}\n\nOutput format:\n{output_format}"}
    ]
)
print(response['message']['content'])

### Analiza wyd≈∫wiƒôku


SYSTEM = "Jeste≈õ precyzyjnym analizatorem wyd≈∫wiƒôku tekstu. Zwracaj wy≈ÇƒÖcznie poprawny JSON wed≈Çug podanej specyfikacji."

prompt = """Okre≈õl wyd≈∫wiƒôk (pozytywny/negatywny/neutralny) poni≈ºszej recenzji.
Zwr√≥ƒá JSON z dwoma kluczami: "sentiment" i "evidence" (zacytuj decydujƒÖcy fragment).

Recenzja:
{imo≈Çszynal_tekst}

"""

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": prompt}
    ]
)
print(response['message']['content'])
SYSTEM = "Jeste≈õ precyzyjnym analizatorem wyd≈∫wiƒôku tekstu. Zwracaj wy≈ÇƒÖcznie poprawny JSON wed≈Çug podanej specyfikacji."

shots = [
    {
        "role": "user",
        "content": "Tekst: \"≈öwietny telefon, bardzo polecam!\""
    },
    {
        "role": "assistant",
        "content": "{\"sentiment\":\"positive\",\"evidence\":\"≈öwietny telefon, bardzo polecam!\"}"
    },
    {
        "role": "user",
        "content": "Tekst: \"Niestety jako≈õƒá wykonania pozostawia wiele do ≈ºyczenia.\""
    },
    {
        "role": "assistant",
        "content": "{\"sentiment\":\"negative\",\"evidence\":\"jako≈õƒá wykonania pozostawia wiele do ≈ºyczenia\"}"
    },
    {
        "role": "user",
        "content": "Tekst: \"{imo≈Çszynal_tekst}\""
    }
]

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        *shots
    ]
)
print(response['message']['content'])
### Ekstrakcja relacji


SYSTEM = "Jeste≈õ precyzyjnym analizatorem tekstu. Znajd≈∫ wszystkie relacje gdzie osoba pracuje dla organizacji. Zwr√≥ƒá tablicƒô JSON z obiektami {\"person\":\"<imiƒô>\",\"company\":\"<organizacja>\"}."

shots = [
    {
        "role": "user", 
        "content": "Tekst: \"Jan Kowalski pracuje w Microsoft jako programista.\""
    },
    {
        "role": "assistant",
        "content": "[{\"person\":\"Jan Kowalski\",\"company\":\"Microsoft\"}]"
    },
    {
        "role": "user",
        "content": "Tekst: \"Anna Kowalska teraz pracuje na AGH, ale kiedy≈õ karierƒô robi≈Ça w Google.\""
    }
]

response = ollama.chat(
    model='gemma2:2b',
    messages=[
        {"role": "system", "content": SYSTEM},
        *shots
    ]
)
print(response['message']['content'])
